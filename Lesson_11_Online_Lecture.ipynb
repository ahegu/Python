{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'> The City University of New York, Queens College\n",
    "\n",
    "<font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'><b>Introduction to Computational Social Science</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>Lesson 11 | Natural Language Processing III: Topic Modeling with the LDA</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>7 Checkpoints</b><br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Begin Lesson 11\n",
    "## Topic Model - The LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is one of the important parts of natural language processing. \n",
    "\n",
    "The goal of topic modeling is to capture the topics from a set of documents and cluster the documents by their topics. Here, a topic is a cluster of terms that happen to co-occur together frequently across documents in a corpus. With many topic models, documents are seen not in terms of their syntax but instead as a mere \"bag of words.\" In other words, documents are just containers for words, where the order they occur doesn't matter. What does matter is that these words co-occured together in the same document. \n",
    "\n",
    "Here we're going to use of the most popular topic models called a LDA (Latent Dirichlet allocation). Since the LDA model doesn’t have to deal with large matrix processing, it has the fastest process speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"LDA\"](Images/12_LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA (short for Latent Dirichlet Allocation) is an unsupervised machine-learning model that takes documents as input and finds topics as output. The model also says in what percentage each document talks about each topic.\n",
    "\n",
    "The mechanics of the LDA model are complex, so let's look at it from a high-level. Assumes you have a corpus made up of documents, and each of these documents are made up of terms. (And like the good computational social scientist, you already stemmed and cleaned-up these terms in each document across the corpus.) The LDA model assumes that the terms of every document is actually a particular mixture of some number of topics, where a topic is characterized by a distribution of terms. \n",
    "\n",
    "Both the topics in a document and words in a topics follow something called a Dirichlet distribution. This distribution is somewhat related to a coin-flip. When you flip a coin, you have one of two options: heads or tails. Now, replace \"heads\" and \"tails\" with the number of topics that exist in your corpus. It's highly unlikely that you have only two topics, so we use the Dirichlet distribution to handle a coin with multiple sides. (Obviously, a coin only has two sides, but I want to make the point that essentially the Dirichelt distribution is a coin-toss with multiple sides.) \n",
    "\n",
    "Thus, a topic is represented as a weighted list of words. An example of a topic is shown below:\n",
    "\n",
    "![\"LDA\"](Images/12_LDA_2.png)\n",
    "\n",
    "The LDA topic model will \"train\" or \"learn\" these data (think back to machine learning!), and determine the probability distribution of every term across all of the topics. (We can also determine from this the probability distribution of documents across these topics.)\n",
    "\n",
    "\n",
    "So, how many topics are there? There is no correct answer of how many topics there are, because you decide how many exist **before** you run the LDA. This is where both the art and science of LDA comes head-to-head. How we choose the number of objects totally dependents on the purpose of the project. However, there are ways to measuer if we picked the number of topics that are a good \"fit\" for our specific data. \n",
    "\n",
    "Thus, there are 3 main parameters of the model:\n",
    "- the number of topics, given by the parameter `K`\n",
    "- `Alpha`, which represents document-topic density. With a higher alpha, documents are made up of more topics, and with lower alpha, documents contain fewer topics. Higher alpha results in a more specific topic distribution per document. \n",
    "- `Beta`, which represents topic-word density. With a higher beta, topics are made up of most of the words in the corpus, and with a low beta they consist of few words. `Beta` results in a more specific word distribution per topic.\n",
    "\n",
    "In reality, the last two parameters are not exactly designed like this in the algorithm, but I prefer to stick to these simplified versions which are easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using `gensim` to run our Topic Models, so let's import them here. \n",
    "\n",
    "First, let's import the modules we'll need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from nltk.corpus import stopwords\n",
    "import nltk \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from dateutil.parser import parse\n",
    "import requests\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this example, let's use a dataset of articles taken from BBC’s website. To implement the LDA in Python, let's use the package `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/articles_bbc_2018_01_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image copyright PA/EPA Image caption Oligarch ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Husband admits killing French jogger\\r\\n\\r\\nTh...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Media playback is unsupported on your device M...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manchester City's Leroy Sane is ruled out for ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image copyright AFP Image caption Sebastien Br...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            articles lang\n",
       "0  Image copyright PA/EPA Image caption Oligarch ...   en\n",
       "1  Husband admits killing French jogger\\r\\n\\r\\nTh...   en\n",
       "2  Media playback is unsupported on your device M...   en\n",
       "3  Manchester City's Leroy Sane is ruled out for ...   en\n",
       "4  Image copyright AFP Image caption Sebastien Br...   en"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove any NAs from our dataset, as many articles many not have any text associated to it. (E.g., the web scraping process is far from perfect, and sometimes we can't extract data.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many rows we dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 1 of 7\n",
    "## Now you try!\n",
    "\n",
    "### For this checkpoint, just read in similar data to the BBC news articles. These data will come from the New York Times. Label the `DataFrame` as `NYTimes_df`. These New York Times article deal with issues related to Silicon Valley and the tech industry in San Francisco. Since these data are really big, we're just going to subset these data and take the first 1000 rows, hence be sure to use `.head(1000)` when reading in these data. \n",
    "\n",
    "    NYTimes_df = pd.read_csv('Data/New_York_Times_SF_All.csv').head(1000)\n",
    "    \n",
    "### Explore the data just as you did for the BBC data.  Here, you'll focus on the `article` column. \n",
    "\n",
    "\n",
    "### Don't forget to remove NAs! \n",
    "\n",
    "### One small caveat (and this may be useful to you for future work you do): Here, we'll remove any document that contains less than 50 characters. Documents with just short sentences, for instance, are not all that informative. So also run the following code below.  \n",
    "\n",
    "    NYTimes_df = NYTimes_df[NYTimes_df['article'].map(len) >= 50]\n",
    "\n",
    "### This code keeps rows (articles) that have a length of greater than 50 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYTimes_df = pd.read_csv('Data/New_York_Times_SF_All.csv').head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>headline</th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/15 0:00</td>\n",
       "      <td>A Reversal for InDinero, a Once-Struggling Acc...</td>\n",
       "      <td>54a4149838f0d80267d2a507</td>\n",
       "      <td>In 2012, InDinero had spent nearly all of the ...</td>\n",
       "      <td>http://www.nytimes.com/2015/01/01/business/sma...</td>\n",
       "      <td>Jessica Mah was 20 when she helped found InDin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/15 0:00</td>\n",
       "      <td>Jim Harbaugh Inspires a Run on Khakis at Michigan</td>\n",
       "      <td>54a461a438f0d83a07dc3eff</td>\n",
       "      <td>The football coach takes his baggy khakis alon...</td>\n",
       "      <td>http://www.nytimes.com/2015/01/01/fashion/jim-...</td>\n",
       "      <td>ANN ARBOR, Mich.  In the final days of the yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/15 0:00</td>\n",
       "      <td>Recycling Electronic Waste Responsibly: Excuse...</td>\n",
       "      <td>54a48b3938f0d83a07dc487c</td>\n",
       "      <td>With more retail chains offering drop-off bins...</td>\n",
       "      <td>http://www.nytimes.com/2015/01/01/technology/p...</td>\n",
       "      <td>MAYBE you replaced old electronics over the ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/15 0:00</td>\n",
       "      <td>What Lies Beneath Takes a Gulp of Air</td>\n",
       "      <td>54a48cd038f0d83a07dc48dd</td>\n",
       "      <td>Flora Grubb introduces minimal steel holders f...</td>\n",
       "      <td>http://www.nytimes.com/2015/01/01/garden/what-...</td>\n",
       "      <td>People love air plants, but it can be a little...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/15 0:00</td>\n",
       "      <td>Standouts in Tech: Drones, Virtual Reality, In...</td>\n",
       "      <td>54a4943438f0d83a07dc4a97</td>\n",
       "      <td>Farhad Manjoo picks four products from 2014 th...</td>\n",
       "      <td>http://www.nytimes.com/2015/01/01/technology/p...</td>\n",
       "      <td>LOTS of cool new technology products come out ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      datetime                                           headline  \\\n",
       "0  1/1/15 0:00  A Reversal for InDinero, a Once-Struggling Acc...   \n",
       "1  1/1/15 0:00  Jim Harbaugh Inspires a Run on Khakis at Michigan   \n",
       "2  1/1/15 0:00  Recycling Electronic Waste Responsibly: Excuse...   \n",
       "3  1/1/15 0:00              What Lies Beneath Takes a Gulp of Air   \n",
       "4  1/1/15 0:00  Standouts in Tech: Drones, Virtual Reality, In...   \n",
       "\n",
       "                         id  \\\n",
       "0  54a4149838f0d80267d2a507   \n",
       "1  54a461a438f0d83a07dc3eff   \n",
       "2  54a48b3938f0d83a07dc487c   \n",
       "3  54a48cd038f0d83a07dc48dd   \n",
       "4  54a4943438f0d83a07dc4a97   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  In 2012, InDinero had spent nearly all of the ...   \n",
       "1  The football coach takes his baggy khakis alon...   \n",
       "2  With more retail chains offering drop-off bins...   \n",
       "3  Flora Grubb introduces minimal steel holders f...   \n",
       "4  Farhad Manjoo picks four products from 2014 th...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://www.nytimes.com/2015/01/01/business/sma...   \n",
       "1  http://www.nytimes.com/2015/01/01/fashion/jim-...   \n",
       "2  http://www.nytimes.com/2015/01/01/technology/p...   \n",
       "3  http://www.nytimes.com/2015/01/01/garden/what-...   \n",
       "4  http://www.nytimes.com/2015/01/01/technology/p...   \n",
       "\n",
       "                                             article  \n",
       "0  Jessica Mah was 20 when she helped found InDin...  \n",
       "1  ANN ARBOR, Mich.  In the final days of the yea...  \n",
       "2  MAYBE you replaced old electronics over the ho...  \n",
       "3  People love air plants, but it can be a little...  \n",
       "4  LOTS of cool new technology products come out ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYTimes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYTimes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYTimes_df = NYTimes_df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYTimes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_df = NYTimes_df[NYTimes_df['article'].map(len) >= 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll need to clean the data. Note here that many of the languages of these articles are NOT in English. For our example, we're only going to focus on English langauge articles, but you can just as easily run LDAs on different languages.\n",
    "\n",
    "Since we don't have data on whether the article was written in English or not, we need to install a pacakge called `langdetect` that will provide a fairly reasonably guess as to what language each article is written in. \n",
    "\n",
    "Use `pip` to install `langdetect` and import that module in. Specifically, we'll use a function called `detect` from this module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /usr/share/pip-wheels\r\n",
      "Requirement already satisfied: langdetect in /home/ahegu/.local/lib/python3.6/site-packages (1.0.8)\r\n",
      "Requirement already satisfied: six in /usr/lib/python3.6/site-packages (from langdetect) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3.6 install --user langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the `apply` method to the `articles` column in our `DataFrame` and apply a function called `detect` from `langdetect`. It will return a two-letter code for the langauge it detects in every article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lang'] = data.articles.apply(detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at our new column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image copyright PA/EPA Image caption Oligarch ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Husband admits killing French jogger\\r\\n\\r\\nTh...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Media playback is unsupported on your device M...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manchester City's Leroy Sane is ruled out for ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image copyright AFP Image caption Sebastien Br...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            articles lang\n",
       "0  Image copyright PA/EPA Image caption Oligarch ...   en\n",
       "1  Husband admits killing French jogger\\r\\n\\r\\nTh...   en\n",
       "2  Media playback is unsupported on your device M...   en\n",
       "3  Manchester City's Leroy Sane is ruled out for ...   en\n",
       "4  Image copyright AFP Image caption Sebastien Br...   en"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It seems to mostly work! \n",
    "\n",
    "Now let's use the `pandas` method `.value_counts()` to see how many articles are written in each language. As you can see, `en` (English) is far in the majority. (Given that this is the BBC, this shouldn't come as a surprise.) Since English data are plentiful, we'll uuse them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    256\n",
       "fa      9\n",
       "fr      8\n",
       "id      5\n",
       "hi      4\n",
       "ru      4\n",
       "vi      4\n",
       "uk      4\n",
       "ar      4\n",
       "sw      3\n",
       "tr      2\n",
       "pt      2\n",
       "es      2\n",
       "de      1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.lang.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only want English article, let's subset on the new column we just made that contains the language for each article. Let's subset just on English (or `en`) articles, denoted by `en` in the `lang` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.loc[data.lang=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 2 of 7\n",
    "## Now you try!\n",
    "\n",
    "### Using the New York Times `DataFrame`, count the numbe of articles that are in English, and keep only the articles that are in English. \n",
    "\n",
    "### Note, that you'll focus only on the column called `article`. This may take some time! \n",
    "\n",
    "### Also recall how we removed any article with less than 50 characters. This is because `detect` will only work if there's enough text to use to determine the language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYTimes_df['lang'] = NYTimes_df.article.apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>headline</th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>article</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/15 0:00</td>\n",
       "      <td>A Reversal for InDinero, a Once-Struggling Acc...</td>\n",
       "      <td>54a4149838f0d80267d2a507</td>\n",
       "      <td>In 2012, InDinero had spent nearly all of the ...</td>\n",
       "      <td>http://www.nytimes.com/2015/01/01/business/sma...</td>\n",
       "      <td>Jessica Mah was 20 when she helped found InDin...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/15 0:00</td>\n",
       "      <td>Jim Harbaugh Inspires a Run on Khakis at Michigan</td>\n",
       "      <td>54a461a438f0d83a07dc3eff</td>\n",
       "      <td>The football coach takes his baggy khakis alon...</td>\n",
       "      <td>http://www.nytimes.com/2015/01/01/fashion/jim-...</td>\n",
       "      <td>ANN ARBOR, Mich.  In the final days of the yea...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/15 0:00</td>\n",
       "      <td>Recycling Electronic Waste Responsibly: Excuse...</td>\n",
       "      <td>54a48b3938f0d83a07dc487c</td>\n",
       "      <td>With more retail chains offering drop-off bins...</td>\n",
       "      <td>http://www.nytimes.com/2015/01/01/technology/p...</td>\n",
       "      <td>MAYBE you replaced old electronics over the ho...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/15 0:00</td>\n",
       "      <td>What Lies Beneath Takes a Gulp of Air</td>\n",
       "      <td>54a48cd038f0d83a07dc48dd</td>\n",
       "      <td>Flora Grubb introduces minimal steel holders f...</td>\n",
       "      <td>http://www.nytimes.com/2015/01/01/garden/what-...</td>\n",
       "      <td>People love air plants, but it can be a little...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/15 0:00</td>\n",
       "      <td>Standouts in Tech: Drones, Virtual Reality, In...</td>\n",
       "      <td>54a4943438f0d83a07dc4a97</td>\n",
       "      <td>Farhad Manjoo picks four products from 2014 th...</td>\n",
       "      <td>http://www.nytimes.com/2015/01/01/technology/p...</td>\n",
       "      <td>LOTS of cool new technology products come out ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      datetime                                           headline  \\\n",
       "0  1/1/15 0:00  A Reversal for InDinero, a Once-Struggling Acc...   \n",
       "1  1/1/15 0:00  Jim Harbaugh Inspires a Run on Khakis at Michigan   \n",
       "2  1/1/15 0:00  Recycling Electronic Waste Responsibly: Excuse...   \n",
       "3  1/1/15 0:00              What Lies Beneath Takes a Gulp of Air   \n",
       "4  1/1/15 0:00  Standouts in Tech: Drones, Virtual Reality, In...   \n",
       "\n",
       "                         id  \\\n",
       "0  54a4149838f0d80267d2a507   \n",
       "1  54a461a438f0d83a07dc3eff   \n",
       "2  54a48b3938f0d83a07dc487c   \n",
       "3  54a48cd038f0d83a07dc48dd   \n",
       "4  54a4943438f0d83a07dc4a97   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  In 2012, InDinero had spent nearly all of the ...   \n",
       "1  The football coach takes his baggy khakis alon...   \n",
       "2  With more retail chains offering drop-off bins...   \n",
       "3  Flora Grubb introduces minimal steel holders f...   \n",
       "4  Farhad Manjoo picks four products from 2014 th...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://www.nytimes.com/2015/01/01/business/sma...   \n",
       "1  http://www.nytimes.com/2015/01/01/fashion/jim-...   \n",
       "2  http://www.nytimes.com/2015/01/01/technology/p...   \n",
       "3  http://www.nytimes.com/2015/01/01/garden/what-...   \n",
       "4  http://www.nytimes.com/2015/01/01/technology/p...   \n",
       "\n",
       "                                             article lang  \n",
       "0  Jessica Mah was 20 when she helped found InDin...   en  \n",
       "1  ANN ARBOR, Mich.  In the final days of the yea...   en  \n",
       "2  MAYBE you replaced old electronics over the ho...   en  \n",
       "3  People love air plants, but it can be a little...   en  \n",
       "4  LOTS of cool new technology products come out ...   en  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYTimes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    292\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYTimes_df.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_df = NYTimes_df.loc[NYTimes_df.lang=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we're going to have to do the dirty work of cleaning each article and preparing it for our LDA. Thankfully, we've actually done this process before, so it should be rather easy. \n",
    "\n",
    "If you are a bit shaky on this process, review the previous Lecture Notebook on cleaning data for a good refresher. \n",
    "\n",
    "We're going to remove punctuation and lemmatize these articles, as well as remove any English stop words from it. In this process, we'll convert each article into a list of tokens to be processed by the LDA. \n",
    "\n",
    "Below, let's define the punctuation, lemmatizer, and stop words, as well as the function that will do the heavy lifting that we explored last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_process(doc):\n",
    "    \n",
    "    ## stop words and updates\n",
    "    ## Note, you should add more terms to this list to see what may or may not be useful.\n",
    "    ## Also note, that I also remove punctuation here by adding the string module\n",
    "    stop_en = stopwords.words('english') + list(string.punctuation) + [u'...',u',',u'.',u'?',u'!',u':',u';', u')', u'(',u'[',u']',u'{',u'}',u'%',u'@',u'-',u'`',\n",
    "                                           u'san',u'francisco',u'san francisco',u'new',u'tr',u'th',u'to',u'on',u'of',u'mr',\n",
    "                                           u'monday','tuesday',u'wednesday',u'thursday',u'friday',u'saturday',u'sunday','want','befor','becaus'\n",
    "                                           u'said',u'ms',u'york',u'say',u'could',u'q',u'got',u'found',u'began','|',\"''\",\"'s\",\"``\",\"--\",\n",
    "                                           'mr','year','would','one','way','l','ms.','$','mr.','dr.','get','before','like','know','day','because',\n",
    "                                           '\"','see','look','dont','im','&','b','also','de','la','el','en','un','two','al','su','es','lo','se']\n",
    "    \n",
    "        \n",
    "    #stemming\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    #lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    #tokenize\n",
    "    tokens = [w.strip() for sent in sent_tokenize(doc) for w in word_tokenize(sent)] if doc else None\n",
    "    \n",
    "    #remove numbers\n",
    "    num_pat = re.compile(r'^(-|\\+)?(\\d*).?(\\d+)')\n",
    "    tokens = filter(lambda x: not num_pat.match(x), tokens)\n",
    "    \n",
    "    #remove dates\n",
    "    date_pat =  re.compile(r'^(\\d{1,2})(/|-)(\\d{1,2})(/|-)(\\d{2,4})$')\n",
    "    tokens = filter(lambda x: not date_pat.match(x), tokens)\n",
    "    \n",
    "    #use stemmer\n",
    "    stemmed_tokens = map(lambda x: stemmer.stem(x), tokens)\n",
    "    \n",
    "    #filter out empty tokens and stopwords\n",
    "    stemmed_tokens = filter(lambda x: x and x.strip() not in stop_en, stemmed_tokens)\n",
    "\n",
    "    #use lemmatizer\n",
    "    lemmatized_and_stemmed_tokens = map(lambda x: lemmatizer.lemmatize(x), stemmed_tokens)\n",
    "\n",
    "    #again filter out empty tokens and stopwords\n",
    "    lemmatized_and_stemmed_tokens = filter(lambda x: x and x.strip() not in stop_en, lemmatized_and_stemmed_tokens)\n",
    "\n",
    "    #remove any lingering white space tokens\n",
    "    lemmatized_and_stemmed_tokens = filter(lambda x: x and x.strip() not in [u' '],lemmatized_and_stemmed_tokens)\n",
    "\n",
    "    x = ' '.join(lemmatized_and_stemmed_tokens)\n",
    "    return x.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply `token_process`, a modified (and combined) function from last time that we apply to the articles column. \n",
    "\n",
    "This function will produce tokens that have been put in lower case, lemmatized, and where stop words and punctuations have been removed. Here, I add more stop words and punctuations that may be unique to this corpus. When you actually do this with your own data, you will need to go back and update this list accordingly.  \n",
    "\n",
    "We'll apply `token_process()` to each row in `articles` by using the `.apply(lambda x:)` `pandas` method. The end result is an article that was once one long string now turned into a list of tokens that have been cleaned up and ready for processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"articles_filtered\"] = data[\"articles\"].apply(lambda doc: token_process(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at this new column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [imag, copyright, pa/epa, imag, caption, oliga...\n",
       "1    [husband, admit, kill, french, jogger, three, ...\n",
       "2    [medium, playback, unsupport, devic, medium, c...\n",
       "3    [manchest, citi, leroy, sane, rule, six, seven...\n",
       "4    [imag, copyright, afp, imag, caption, sebastie...\n",
       "Name: articles_filtered, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.articles_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 3 of 7\n",
    "## Now you try!\n",
    "\n",
    "### Filter the articles in just the same way as we did here for the BBC articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_process(doc):\n",
    "    \n",
    "    ## stop words and updates\n",
    "    ## Note, you should add more terms to this list to see what may or may not be useful.\n",
    "    ## Also note, that I also remove punctuation here by adding the string module\n",
    "    stop_en = stopwords.words('english') + list(string.punctuation) + [u'...',u',',u'.',u'?',u'!',u':',u';', u')', u'(',u'[',u']',u'{',u'}',u'%',u'@',u'-',u'`',\n",
    "                                           u'san',u'francisco',u'san francisco',u'new',u'tr',u'th',u'to',u'on',u'of',u'mr',\n",
    "                                           u'monday','tuesday',u'wednesday',u'thursday',u'friday',u'saturday',u'sunday','want','befor','becaus'\n",
    "                                           u'said',u'ms',u'york',u'say',u'could',u'q',u'got',u'found',u'began','|',\"''\",\"'s\",\"``\",\"--\",\n",
    "                                           'mr','year','would','one','way','l','ms.','$','mr.','dr.','get','before','like','know','day','because',\n",
    "                                           '\"','see','look','dont','im','&','b','also','de','la','el','en','un','two','al','su','es','lo','se']\n",
    "    \n",
    "        \n",
    "    #stemming\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    #lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    #tokenize\n",
    "    tokens = [w.strip() for sent in sent_tokenize(doc) for w in word_tokenize(sent)] if doc else None\n",
    "    \n",
    "    #remove numbers\n",
    "    num_pat = re.compile(r'^(-|\\+)?(\\d*).?(\\d+)')\n",
    "    tokens = filter(lambda x: not num_pat.match(x), tokens)\n",
    "    \n",
    "    #remove dates\n",
    "    date_pat =  re.compile(r'^(\\d{1,2})(/|-)(\\d{1,2})(/|-)(\\d{2,4})$')\n",
    "    tokens = filter(lambda x: not date_pat.match(x), tokens)\n",
    "    \n",
    "    #use stemmer\n",
    "    stemmed_tokens = map(lambda x: stemmer.stem(x), tokens)\n",
    "    \n",
    "    #filter out empty tokens and stopwords\n",
    "    stemmed_tokens = filter(lambda x: x and x.strip() not in stop_en, stemmed_tokens)\n",
    "\n",
    "    #use lemmatizer\n",
    "    lemmatized_and_stemmed_tokens = map(lambda x: lemmatizer.lemmatize(x), stemmed_tokens)\n",
    "\n",
    "    #again filter out empty tokens and stopwords\n",
    "    lemmatized_and_stemmed_tokens = filter(lambda x: x and x.strip() not in stop_en, lemmatized_and_stemmed_tokens)\n",
    "\n",
    "    #remove any lingering white space tokens\n",
    "    lemmatized_and_stemmed_tokens = filter(lambda x: x and x.strip() not in [u' '],lemmatized_and_stemmed_tokens)\n",
    "\n",
    "    x = ' '.join(lemmatized_and_stemmed_tokens)\n",
    "    return x.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYTimes_df[\"article_filtered\"] = NYTimes_df[\"article\"].apply(lambda doc: token_process(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [jessica, mah, help, indinero, back, believ, h...\n",
       "1    [ann, arbor, mich, final, rumor, intensifi, ji...\n",
       "2    [mayb, replac, old, electron, holiday, sweep, ...\n",
       "3    [peopl, love, air, plant, littl, tricki, figur...\n",
       "4    [lot, cool, technolog, product, come, everi, u...\n",
       "Name: article_filtered, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYTimes_df.article_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Let's run the LDA!\n",
    "\n",
    "We'll need to turn this column into a format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, convert this column to a `list` using the `.tolist()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_corpus = data[\"articles_filtered\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the module `corpora`, we'll use the function `Dictionary` to convert our list of tokens into a `corpora-specific Dictionary` object. We do this because we want to do some \"trimming\" of the corpus that is more easily done (e.g., doesn't take up too much memory) in this format. \n",
    "\n",
    "(Indeed, the biggest challenge with text analysis is memory constraints, so converting objects into other form may be annoying, but is a necessity to avoid a memory leakage issue, halting your code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_LDA = corpora.Dictionary(initial_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the words are going to be superfluous: one off words that don't have any meaning that we didn't catch with the stop words. This is rather common, as capturing every token that doesn't have any meaning is near impossible. \n",
    "\n",
    "Instead, we can create a cutoff value that removes terms if they appear less than `n` number of times in the corpus. Here, we can usee the `.filter_extremes()` method applied to our `corpora Dictionary`. \n",
    "\n",
    "I set it to 3, so any term that occurs in less than three documents will be removed. \n",
    "\n",
    "**Note:** This is a bit confusing, but we don't need to save this filtering onto itself. It automatically does it in place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_LDA.filter_extremes(no_below=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the corpus before we submit it to the LDA model. \n",
    "\n",
    "Recall that the LDA model sees documents as a \"bag of words.\" In other words, documents are a big burlap sack of words, and the fact that these words were used in this document means something. Namely, that documents are themselves just collections of topics. We just don't know how many topics and which terms are in these topics. This is the job of the LDA to uncover!\n",
    "\n",
    "For the package `gensim`, we can't merely pass in the list of tokens in its current form. Instead we need to pass it in a very particular format, namely as a list of tuples. In other words, each element in the tuple is a combination of the token: as a unique identifier given as some number, followed by the number of times it appears in this specific document. \n",
    "\n",
    "Let's actually see what this means. First, let's convert our list of documents currently stored as a list of tokens (again, this is found in `initial_corpus`) into a list of tuples of token IDs and the number of times the token appears in the document. \n",
    "\n",
    "We'll use the method `.doc2bow()`---in other words, document to \"bag of words\"---to convert each \"document\" in the list `initial_corpus` to convert the string tokens into these tuples. \n",
    "\n",
    "So, we use list comprehension to loop through each article in `initial_corpus` and convert every list to a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary_LDA.doc2bow(doc_) for doc_ in initial_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's see the first document in the corpus, for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 4),\n",
       " (3, 2),\n",
       " (4, 2),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 2),\n",
       " (11, 1),\n",
       " (12, 2),\n",
       " (13, 2),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 2),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 2),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 2),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 2),\n",
       " (38, 2),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 3),\n",
       " (42, 3),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 3),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 2),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 4),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 2),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 2),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 3),\n",
       " (73, 1),\n",
       " (74, 3),\n",
       " (75, 2),\n",
       " (76, 1),\n",
       " (77, 2),\n",
       " (78, 1),\n",
       " (79, 2),\n",
       " (80, 3),\n",
       " (81, 2),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 2),\n",
       " (85, 1),\n",
       " (86, 2),\n",
       " (87, 1),\n",
       " (88, 3),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 3),\n",
       " (92, 1),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 3),\n",
       " (99, 2),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 3),\n",
       " (103, 5),\n",
       " (104, 1),\n",
       " (105, 1),\n",
       " (106, 2),\n",
       " (107, 5),\n",
       " (108, 2),\n",
       " (109, 3),\n",
       " (110, 3),\n",
       " (111, 1),\n",
       " (112, 3),\n",
       " (113, 3),\n",
       " (114, 1),\n",
       " (115, 1),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 1),\n",
       " (119, 1),\n",
       " (120, 2),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 2),\n",
       " (125, 1),\n",
       " (126, 6),\n",
       " (127, 1),\n",
       " (128, 7),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 20),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 3),\n",
       " (137, 2),\n",
       " (138, 1),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 3),\n",
       " (147, 2),\n",
       " (148, 2),\n",
       " (149, 1),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 1),\n",
       " (153, 1),\n",
       " (154, 1),\n",
       " (155, 1),\n",
       " (156, 3),\n",
       " (157, 2),\n",
       " (158, 10),\n",
       " (159, 1),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 2),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 1),\n",
       " (167, 3),\n",
       " (168, 1),\n",
       " (169, 5),\n",
       " (170, 1),\n",
       " (171, 2),\n",
       " (172, 1),\n",
       " (173, 3),\n",
       " (174, 1),\n",
       " (175, 1),\n",
       " (176, 1),\n",
       " (177, 1),\n",
       " (178, 2),\n",
       " (179, 2),\n",
       " (180, 1),\n",
       " (181, 6),\n",
       " (182, 2),\n",
       " (183, 1),\n",
       " (184, 2),\n",
       " (185, 2),\n",
       " (186, 3),\n",
       " (187, 1),\n",
       " (188, 2),\n",
       " (189, 9),\n",
       " (190, 1),\n",
       " (191, 1),\n",
       " (192, 1),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 1),\n",
       " (196, 1),\n",
       " (197, 3),\n",
       " (198, 1),\n",
       " (199, 1),\n",
       " (200, 2),\n",
       " (201, 1),\n",
       " (202, 1),\n",
       " (203, 1),\n",
       " (204, 2),\n",
       " (205, 1),\n",
       " (206, 9),\n",
       " (207, 12),\n",
       " (208, 9),\n",
       " (209, 15),\n",
       " (210, 1),\n",
       " (211, 2),\n",
       " (212, 1),\n",
       " (213, 2),\n",
       " (214, 1),\n",
       " (215, 2),\n",
       " (216, 1),\n",
       " (217, 1),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 2),\n",
       " (224, 1),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 1),\n",
       " (229, 2),\n",
       " (230, 1),\n",
       " (231, 1),\n",
       " (232, 2),\n",
       " (233, 1),\n",
       " (234, 4),\n",
       " (235, 2),\n",
       " (236, 1),\n",
       " (237, 4),\n",
       " (238, 5),\n",
       " (239, 1),\n",
       " (240, 14),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 1),\n",
       " (248, 1),\n",
       " (249, 1),\n",
       " (250, 2),\n",
       " (251, 1),\n",
       " (252, 2)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see a list of tuples, where the first entry are the unique IDs assigned to each token, followed by the number of times the token appears in the first document. \n",
    "\n",
    "Let's see how this corresponds to what we have in our `initial_corpus`. \n",
    "\n",
    "First, let's look at the first entry.\n",
    "\n",
    "We can use `dictionary_LDA` to convert from a token ID to the actual token. So let's try that out. \n",
    "\n",
    "Look at the third ID entry (i.e., value of 2) and its actual value (i.e., not the ID).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'act'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_LDA[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the raw data from `initial_corpus`, looking at the same document, the number of times this token occurs is the second value in the tuple. \n",
    "\n",
    "(Check for yourself! To do this, use `Control-F` or `Command-F` [depending on your OS] in your browser and search for the term `listed`. It should appear twice in the long list below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even do this with the original article. Print out the original raw article and find the number of times `listed` appears. It should be twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image copyright PA/EPA Image caption Oligarch Roman Abramovich (l) and PM Dmitry Medvedev are on the list\r\n",
      "\r\n",
      "Russian President Vladimir Putin says a list of officials and businessmen close to the Kremlin published by the US has in effect targeted all Russian people.\r\n",
      "\r\n",
      "The list names 210 top Russians as part of a sanctions law aimed at punishing Moscow for meddling in the US election.\r\n",
      "\r\n",
      "However, the US stressed those named were not subject to new sanctions.\r\n",
      "\r\n",
      "Mr Putin said the list was an unfriendly act that complicated US-Russia ties but he said he did not want to escalate the situation.\r\n",
      "\r\n",
      "Mr Putin said Russia should instead be thinking about \"ourselves and the economy\".\r\n",
      "\r\n",
      "The list was also derided by a number of senior Russian officials who said it bore a strong resemblance to the Forbes magazine ranking of Russian billionaires. A US Treasury Department later told Buzzfeed that an unclassified annex of the report had been derived from the magazine.\r\n",
      "\r\n",
      "Why did the US publish the list?\r\n",
      "\r\n",
      "The government was required to draw up the list after Congress passed the Countering America's Adversaries Through Sanctions Act (Caatsa) in August.\r\n",
      "\r\n",
      "The law aimed to punish Russia for its alleged meddling in the 2016 US presidential election and its actions in Ukraine.\r\n",
      "\r\n",
      "Congress wanted the list to name and shame those who had benefited from close association with President Putin and put them on notice that they could be targeted for sanctions, or more sanctions, in the future.\r\n",
      "\r\n",
      "President Donald Trump did not support Caatsa, even though he signed it into law, saying it was \"unconstitutional\".\r\n",
      "\r\n",
      "Under the law, the list had to be delivered by Monday. The fact it was released about 10 minutes before midnight may reflect Mr Trump's coolness towards it, and his opposition to punishing more Russians with sanctions.\r\n",
      "\r\n",
      "The top Democrat on the House Foreign Affairs Committee, Eliot Engel, accused the Trump administration of letting \"Russia off the hook again\" by not taking substantial action.\r\n",
      "\r\n",
      "Who has been named?\r\n",
      "\r\n",
      "Informally known as the \"Putin list\", the unclassified section has 210 names, 114 of them in the government or linked to it, or key businessmen. The other 96 are oligarchs apparently determined more by the fact they are worth more than $1bn (£710m) than their close ties to the Kremlin.\r\n",
      "\r\n",
      "Image copyright Reuters Image caption Congress passed the law in August, although President Donald Trump had opposed it\r\n",
      "\r\n",
      "Most of Mr Putin's longstanding allies are named, many of them siloviki (security guys). They include the spy chiefs Alexander Bortnikov of the Federal Security Service (FSB) - which Mr Putin used to run - and Sergei Naryshkin of the Foreign Intelligence Service (SVR).\r\n",
      "\r\n",
      "The men who control Russia's energy resources are listed: Gazprom chief Alexei Miller, Rosneft chief Igor Sechin and other oil and gas executives, along with top bankers like Bank Rossiya manager Yuri Kovalchuk.\r\n",
      "\r\n",
      "The oligarchs include Kirill Shamalov, who is reported to be Mr Putin's son-in-law, although the Kremlin has never confirmed his marriage to Katerina Tikhonova, nor even that she is the president's daughter.\r\n",
      "\r\n",
      "Internationally known oligarchs are there too, such as those with stakes in top English football clubs: Alisher Usmanov (Arsenal) and Roman Abramovich (Chelsea).\r\n",
      "\r\n",
      "Will they face new sanctions?\r\n",
      "\r\n",
      "Not at the moment. The US Treasury document itself stresses: \"It is not a sanctions list, and the inclusion of individuals or entities... does not and in no way should be interpreted to impose sanctions on those individuals or entities.\"\r\n",
      "\r\n",
      "It adds: \"Neither does inclusion on the unclassified list indicate that the US government has information about the individual's involvement in malign activities.\"\r\n",
      "\r\n",
      "However, there is a classified version said to include information detailing allegations of involvement in corrupt activities.\r\n",
      "\r\n",
      "What does it mean for Russia's elite?\r\n",
      "\r\n",
      "Analysis: Steve Rosenberg, BBC Moscow correspondent\r\n",
      "\r\n",
      "The good news for the Kremlin: this isn't a sanctions list. But the good news ends there.\r\n",
      "\r\n",
      "Those Russian officials and oligarchs named by the US Treasury will worry that their inclusion could signal sanctions in the future.\r\n",
      "\r\n",
      "Even before the list was made public, the Kremlin had claimed the US Treasury report was an attempt to meddle in Russia's presidential election.\r\n",
      "\r\n",
      "The list reads like a Who's Who of the Russian political elite and business world.\r\n",
      "\r\n",
      "Moscow won't want that to become a Who's Sanctioned.\r\n",
      "\r\n",
      "What is the Caatsa act and did the president want it?\r\n",
      "\r\n",
      "The law limited the amount of money Americans could invest in Russian energy projects and made it more difficult for US companies to do business with Russia.\r\n",
      "\r\n",
      "It also imposed sanctions on Iran and North Korea.\r\n",
      "\r\n",
      "Media playback is unsupported on your device Media caption All you need to know about the Trump-Russia investigation\r\n",
      "\r\n",
      "In signing the act, Mr Trump attached a statement calling the measure \"deeply flawed\" and said he could make \"far better deals with foreign countries than Congress\".\r\n",
      "\r\n",
      "Earlier on Monday, the US government argued the Caatsa law had already pushed governments around the world to cancel deals with Russia worth billions, suggesting that more sanctions were not required.\r\n",
      "\r\n",
      "How have the Russians reacted?\r\n",
      "\r\n",
      "Perhaps referring to the fact that all of their political representatives had been named, Mr Putin said that, in effect, \"all 146 million Russians have been put on the list\".\r\n",
      "\r\n",
      "He joked he was offended not to be named himself.\r\n",
      "\r\n",
      "Earlier, Kremlin spokesman Dmitry Peskov, who is himself on the list, accepted that it was not one of sanctions but said it could potentially damage \"the image and reputation\" of figures listed and their associated companies.\r\n",
      "\r\n",
      "He added: \"It's not the first day that we live with quite aggressive comments made towards us, so we should not give in to emotions.\"\r\n",
      "\r\n",
      "When Caatsa was passed, PM Dmitry Medvedev said it meant the US had declared a \"full-scale trade war\" on Russia.\r\n",
      "\r\n",
      "Russian opposition leader Alexei Navalny praised the publication of the names as \"a good list\".\n"
     ]
    }
   ],
   "source": [
    "print(data.articles.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 4 of 7\n",
    "## Now you try!\n",
    "\n",
    "### Prepare the `Dictionary`, `corpus`, and the `initial_corpus` for the New York Times data. \n",
    "\n",
    "### Label the `Dictionary` as `NYTimes_dictionary_LDA`, the `corpus` as `NYTimes_corpus`, and the `initial_corpus` as `NYTimes_initial_corpus`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " NYTimes_initial_corpus = NYTimes_df[\"article_filtered\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_dictionary_LDA = corpora.Dictionary(NYTimes_initial_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_dictionary_LDA.filter_extremes(no_below=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_corpus = [NYTimes_dictionary_LDA.doc2bow(doc_) for doc_ in  NYTimes_initial_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6),\n",
       " (1, 6),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 3),\n",
       " (12, 5),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 4),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 3),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 2),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 3),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 2),\n",
       " (31, 2),\n",
       " (32, 2),\n",
       " (33, 1),\n",
       " (34, 8),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 4),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 3),\n",
       " (42, 3),\n",
       " (43, 1),\n",
       " (44, 2),\n",
       " (45, 1),\n",
       " (46, 5),\n",
       " (47, 2),\n",
       " (48, 2),\n",
       " (49, 11),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 2),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 7),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 4),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 4),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 2),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 2),\n",
       " (90, 2),\n",
       " (91, 2),\n",
       " (92, 4),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 2),\n",
       " (99, 1),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 1),\n",
       " (104, 1),\n",
       " (105, 3),\n",
       " (106, 2),\n",
       " (107, 2),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 1),\n",
       " (111, 2),\n",
       " (112, 1),\n",
       " (113, 1),\n",
       " (114, 2),\n",
       " (115, 3),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 2),\n",
       " (119, 1),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 5),\n",
       " (127, 1),\n",
       " (128, 4),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 2),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 3),\n",
       " (138, 2),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 3),\n",
       " (149, 1),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 2),\n",
       " (153, 2),\n",
       " (154, 1),\n",
       " (155, 2),\n",
       " (156, 1),\n",
       " (157, 1),\n",
       " (158, 1),\n",
       " (159, 1),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 1),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 3),\n",
       " (166, 1),\n",
       " (167, 3),\n",
       " (168, 1),\n",
       " (169, 1),\n",
       " (170, 1),\n",
       " (171, 4),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 6),\n",
       " (175, 2),\n",
       " (176, 1),\n",
       " (177, 2),\n",
       " (178, 1),\n",
       " (179, 2),\n",
       " (180, 1),\n",
       " (181, 1),\n",
       " (182, 1),\n",
       " (183, 2),\n",
       " (184, 1),\n",
       " (185, 2),\n",
       " (186, 3),\n",
       " (187, 1),\n",
       " (188, 3),\n",
       " (189, 1),\n",
       " (190, 1),\n",
       " (191, 1),\n",
       " (192, 1),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 2),\n",
       " (196, 1),\n",
       " (197, 1),\n",
       " (198, 1),\n",
       " (199, 1),\n",
       " (200, 1),\n",
       " (201, 1),\n",
       " (202, 4),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 1),\n",
       " (206, 1),\n",
       " (207, 1),\n",
       " (208, 1),\n",
       " (209, 5),\n",
       " (210, 1),\n",
       " (211, 4),\n",
       " (212, 1),\n",
       " (213, 1),\n",
       " (214, 1),\n",
       " (215, 1),\n",
       " (216, 3),\n",
       " (217, 1),\n",
       " (218, 1),\n",
       " (219, 2),\n",
       " (220, 3),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 2),\n",
       " (224, 2),\n",
       " (225, 3),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 1),\n",
       " (229, 2),\n",
       " (230, 1),\n",
       " (231, 1),\n",
       " (232, 1),\n",
       " (233, 2),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 3),\n",
       " (237, 4),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 2),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 2),\n",
       " (244, 2),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 1),\n",
       " (248, 1),\n",
       " (249, 1),\n",
       " (250, 1),\n",
       " (251, 1),\n",
       " (252, 1),\n",
       " (253, 5),\n",
       " (254, 2),\n",
       " (255, 2),\n",
       " (256, 1),\n",
       " (257, 1),\n",
       " (258, 1),\n",
       " (259, 2),\n",
       " (260, 3),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 1),\n",
       " (264, 2),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (267, 1),\n",
       " (268, 1),\n",
       " (269, 4),\n",
       " (270, 1),\n",
       " (271, 2),\n",
       " (272, 2),\n",
       " (273, 3),\n",
       " (274, 4),\n",
       " (275, 2),\n",
       " (276, 1),\n",
       " (277, 1),\n",
       " (278, 2),\n",
       " (279, 1),\n",
       " (280, 3),\n",
       " (281, 1),\n",
       " (282, 1),\n",
       " (283, 1),\n",
       " (284, 2),\n",
       " (285, 1),\n",
       " (286, 1),\n",
       " (287, 2),\n",
       " (288, 2),\n",
       " (289, 2),\n",
       " (290, 1),\n",
       " (291, 2),\n",
       " (292, 2),\n",
       " (293, 1),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 2),\n",
       " (297, 1),\n",
       " (298, 1),\n",
       " (299, 1),\n",
       " (300, 2),\n",
       " (301, 2),\n",
       " (302, 1),\n",
       " (303, 1),\n",
       " (304, 1),\n",
       " (305, 2),\n",
       " (306, 1),\n",
       " (307, 1),\n",
       " (308, 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYTimes_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acquisit'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYTimes_dictionary_LDA[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYTimes_corpus[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jessica Mah was 20 when she helped found InDinero in 2009. Back then she believed she could help small-business owners track their finances with her start-ups software. But as it turned out, Ms. Mah could barely keep InDinero afloat, let alone help others run their businesses. In its first version, InDinero, in San Francisco, charged its few customers $20 a month for its software. Competition, which included companies like QuickBooks, was fierce, and a series of bad hires added to InDineros woes. The company was going nowhere, Ms. Mah said. But by the time she realized this, in 2012, InDinero had spent nearly all of the $1.2 million it had raised from investors. A few years ago, I really didnt know that much, said Ms. Mah, the chief executive. In fact, she said, I almost crashed the company. And yet, today, InDinero, which provides accounting software and services to small businesses, has 75 employees and just under $3 million in annual sales. It expects to double that in 2015, and has raised a total of $8 million from investors. In a recent conversation, which has been edited and condensed, Ms. Mah talked about how she turned things around. Q. What were the first signs of a problem? A. I tracked the metrics  the number of sign-ups, upgrades and cancellations. The numbers that should have been going up werent. Q. How close did you come to running out of money? A. We were down to the last $150,000 of the $1.2 million we had raised, and were burning $80,000 to $100,000 a month. Once we got down to about $250,000 we knew we had to dial back. Q. What steps did you take once you realized InDinero was in serious trouble? A. We got rid of our office and employees so we had no expenses. My co-founder, Andy Su, and I moved into an apartment together and got our parents to cover our rent and food. I would have moved home, but my family is in New York, and the company is in California. Then, for most of 2012, we tried to figure out what magical feature we could build to fix InDinero. Q. Did you figure it out? A. I started asking, What would InDinero look like if it were a $1 billion-plus company? Looking at companies like Salesforce and NetSuite, I learned a few things. They all solve a big business problem that justifies a price point higher than $20 a month. They have seasoned executives and well-trained sales teams. Theyre nearly 100 percent premium, with very limited free offerings. Q. How did this help you change your product? A. We knew we needed a product that we could charge a few hundred dollars a month for. I spent countless hours interviewing entrepreneurs of all different shapes and sizes to figure out their accounting and tax needs. We knew they wanted a one-stop accounting solution. It was a big problem for them that we didnt file their taxes. We had to go from offering a cheap software solution that didnt actually solve any problems to being an all-in-one accounting back office with accountants on staff. We had to do it all, including taxes and payroll. Q. How big a business can your software handle  especially when it comes to filing taxes? A. Our 500 customers range from a two-employee start-up with no sales to a 100-employee company with eight-figure sales. Q. What do you charge? A. Businesses pay between $400 and $5,000 per month, depending on how complicated their accounting is. Q. How are you marketing your service? A. Weve found the best way to get new business is to incentivize our current customers. For every referral a customer gives us, we give them a free month. Weve found that, out of five customers, four wont send any referrals, but one will send 50. Q. Once you had the right product, how did you go about hiring the right people? A. I thought about the mistakes I made the first time. I realized I had hired too many of my friends. I should have spent more time evaluating candidates outside my network  expanding the candidate pool through external recruiters, LinkedIn, specialty job boards and other methods. My original interview process wasnt thorough. I had no defined criteria to evaluate candidates against. Q. Do you have any favorite interview questions? A. Yes, I ask the same ones to every single candidate. Some are, Whats the hardest youve ever worked in your life, whats your most lofty ambition and what are you doing for self-development? I want to know what books and blogs they read, what conferences they go to, whether theyre working on side projects in their fields and whether theyve ever run a company. I want a company full of mini-C.E.O.s. I also ask about their relationship with their last boss. If they talk about how horrible their last boss was, Im done. Q. Has your management style changed since InDineros early days? A. Yes, Im focused on going from being a C.E.O. who did the work herself to an effective leader. Im taking the personal growth thing very seriously. Ive hired executive coaches and joined Y.P.O. [Young Presidents Organization]. Last year, I read more than 100 leadership books. Q. Is it hard to let go of the day-to-day? A. In the past, I didnt spend enough time recruiting for senior leadership. I tried to manage 20 people all by myself with no strong managers to grow the company. Now, I dont spend any time having individual contributors report to me. I go straight to finding a strong V.P./director-level person who can build the team out for me. Q. Did InDineros early problems test your relationship with your co-founder? A. Andy and I have always had a strong relationship. Were good friends  and still roommates. Living together is key because we can work on business challenges at all hours of the day. I dont see this changing anytime soon. Q. Whats the division of labor between you? A. Im responsible for customer happiness and customer acquisition. Andy, the chief technology officer, is responsible for product and engineering. We split all the other business functions  like legal, finance and operations  50-50. Q. How do you resolve disputes? A. If we disagree on how to deal with something, we ask the other person how strongly they feel on a 1-to-10 scale about that particular issue. The person who cares more will make the decision. We also see an executive coach to help mediate our disagreements. On top of that, we go to co-founder marriage counseling! [The co-founders are not romantically involved.] One of our core company values is rethink the obvious, so I got the idea to reach out to marriage counselors listed on Yelp to see if any would work with us. Even though marriage counseling is usually for dysfunctional couples, co-founders often have the same petty debates. We wanted to be proactive. Q. What have you learned about early success  and failure? A. In the beginning, because of my age, I got a lot of wunderkind attention. Its important to stay humble and not get carried away by early success  but the same is true of failure. Neither necessarily lasts. So, when InDinero was in trouble, I just talked to my parents and friends, lay low and focused on results.\n"
     ]
    }
   ],
   "source": [
    "print(NYTimes_df.article.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Running the LDA\n",
    "\n",
    "Now that we've done heavy lifting (i.e., formating our data), we can now run the LDA. This is the easy part! \n",
    "\n",
    "Recall that we need to pick out the proper hyperparameters. For our purposes, we'll focus on two here: `K` topics and `alpha`. `K` is more of a tunning parameter, in that it's a bit of a guess as to what the right value of `K` is. \n",
    "\n",
    "For the sake of argument, let's just use a `K` of 20, or that we expect there to be 20 distinct topics in our BBC article corpus. For `alpha`, let's set it as 0.01, a standard value often used in NLP. For `alpha`, we need to pass it in as a list, whose length is the same as the number of topics. This is achieved by `[0.01]*number_of_topics`. \n",
    "\n",
    "We also pass in our \"dictionary\" that we created earlier: `dictionary_LDA` as our token ID to terms dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 20 # Number of Topics, we set initially as K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's FINALLY run it! \n",
    "\n",
    "Pass in `corpus`, the number of topics, the `dictionary_LDA` and `alpha`. LDAs can take some time. To speed this up, we're going to use a special version of the LDA model called a `LdaMulticore` model. It's an LDA model that uses multicore processors (if available) to speed up the model! (You can use the regular `models.LDA()` as well to similar effect.)\n",
    "\n",
    "**NOTE:** This may throw a lot of errors at you and may take a few minutes. Don't freak out at first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaMulticore(corpus,\\\n",
    "                            num_topics=num_topics,\\\n",
    "                                  id2word=dictionary_LDA,\\\n",
    "                                  alpha=[0.01]*num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hopefully if your computer didn't blow up, the model should have completed successfully! Let's take a look at the top  20 words most associated to each of the 20 topics. \n",
    "\n",
    "We'll run a `for` loop and use the method `.show_topics()` to print out the top 20 terms for each of the 20 topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.008*\"’\" + 0.008*\"imag\" + 0.007*\"bbc\" + 0.007*\"said\" + 0.005*\"“\" + 0.005*\"”\" + 0.004*\"peopl\" + 0.004*\"go\" + 0.004*\"time\" + 0.004*\"n't\" + 0.004*\"caption\" + 0.004*\"group\" + 0.004*\"take\" + 0.004*\"use\" + 0.003*\"thing\" + 0.003*\"onli\" + 0.003*\"news\" + 0.003*\"come\" + 0.003*\"work\" + 0.003*\"first\"\n",
      "\n",
      "1: 0.010*\"said\" + 0.007*\"imag\" + 0.006*\"stori\" + 0.005*\"life\" + 0.005*\"restaur\" + 0.005*\"”\" + 0.005*\"polic\" + 0.004*\"“\" + 0.004*\"caption\" + 0.004*\"bbc\" + 0.004*\"kill\" + 0.004*\"copyright\" + 0.004*\"world\" + 0.004*\"told\" + 0.004*\"even\" + 0.004*\"come\" + 0.004*\"find\" + 0.004*\"moment\" + 0.003*\"three\" + 0.003*\"dure\"\n",
      "\n",
      "2: 0.011*\"imag\" + 0.010*\"said\" + 0.006*\"beetl\" + 0.005*\"peopl\" + 0.005*\"first\" + 0.005*\"u\" + 0.004*\"world\" + 0.004*\"caption\" + 0.004*\"time\" + 0.004*\"presid\" + 0.004*\"use\" + 0.004*\"live\" + 0.004*\"’\" + 0.004*\"copyright\" + 0.004*\"anim\" + 0.004*\"help\" + 0.004*\"trump\" + 0.004*\"report\" + 0.004*\"work\" + 0.003*\"plan\"\n",
      "\n",
      "3: 0.019*\"imag\" + 0.013*\"said\" + 0.009*\"u\" + 0.008*\"caption\" + 0.008*\"copyright\" + 0.006*\"peopl\" + 0.005*\"time\" + 0.005*\"n't\" + 0.005*\"use\" + 0.005*\"work\" + 0.004*\"model\" + 0.004*\"first\" + 0.004*\"bbc\" + 0.003*\"presid\" + 0.003*\"go\" + 0.003*\"stori\" + 0.003*\"light\" + 0.003*\"last\" + 0.003*\"compani\" + 0.003*\"make\"\n",
      "\n",
      "4: 0.011*\"imag\" + 0.008*\"said\" + 0.006*\"caption\" + 0.006*\"n't\" + 0.005*\"go\" + 0.005*\"peopl\" + 0.004*\"time\" + 0.004*\"u\" + 0.004*\"copyright\" + 0.004*\"’\" + 0.004*\"told\" + 0.004*\"bbc\" + 0.004*\"world\" + 0.004*\"make\" + 0.004*\"trump\" + 0.004*\"back\" + 0.003*\"may\" + 0.003*\"come\" + 0.003*\"call\" + 0.003*\"first\"\n",
      "\n",
      "5: 0.010*\"imag\" + 0.008*\"said\" + 0.007*\"bbc\" + 0.007*\"n't\" + 0.006*\"radio\" + 0.005*\"use\" + 0.005*\"copyright\" + 0.005*\"caption\" + 0.005*\"u\" + 0.004*\"first\" + 0.004*\"live\" + 0.004*\"think\" + 0.004*\"report\" + 0.004*\"news\" + 0.004*\"work\" + 0.003*\"peopl\" + 0.003*\"show\" + 0.003*\"world\" + 0.003*\"around\" + 0.003*\"flight\"\n",
      "\n",
      "6: 0.009*\"imag\" + 0.007*\"“\" + 0.007*\"’\" + 0.006*\"u\" + 0.006*\"use\" + 0.005*\"said\" + 0.005*\"”\" + 0.005*\"caption\" + 0.004*\"live\" + 0.004*\"wave\" + 0.004*\"back\" + 0.004*\"time\" + 0.004*\"copyright\" + 0.004*\"first\" + 0.003*\"light\" + 0.003*\"everi\" + 0.003*\"bbc\" + 0.003*\"world\" + 0.003*\"may\" + 0.003*\"n't\"\n",
      "\n",
      "7: 0.010*\"imag\" + 0.008*\"use\" + 0.007*\"bbc\" + 0.007*\"u\" + 0.007*\"list\" + 0.006*\"news\" + 0.006*\"caption\" + 0.005*\"said\" + 0.005*\"time\" + 0.005*\"report\" + 0.004*\"help\" + 0.004*\"’\" + 0.004*\"includ\" + 0.004*\"n't\" + 0.003*\"medium\" + 0.003*\"inform\" + 0.003*\"call\" + 0.003*\"copyright\" + 0.003*\"make\" + 0.003*\"social\"\n",
      "\n",
      "8: 0.011*\"said\" + 0.009*\"u\" + 0.008*\"imag\" + 0.007*\"’\" + 0.005*\"bbc\" + 0.005*\"n't\" + 0.004*\"use\" + 0.004*\"caption\" + 0.004*\"time\" + 0.004*\"back\" + 0.004*\"–\" + 0.004*\"list\" + 0.004*\"russia\" + 0.004*\"govern\" + 0.003*\"take\" + 0.003*\"“\" + 0.003*\"”\" + 0.003*\"come\" + 0.003*\"copyright\" + 0.003*\"world\"\n",
      "\n",
      "9: 0.025*\"’\" + 0.016*\"”\" + 0.013*\"“\" + 0.007*\"use\" + 0.006*\"said\" + 0.006*\"–\" + 0.006*\"time\" + 0.005*\"imag\" + 0.005*\"peopl\" + 0.004*\"citi\" + 0.004*\"make\" + 0.004*\"onli\" + 0.004*\"bbc\" + 0.004*\"come\" + 0.004*\"work\" + 0.004*\"world\" + 0.004*\"u\" + 0.004*\"light\" + 0.003*\"first\" + 0.003*\"around\"\n",
      "\n",
      "10: 0.026*\"imag\" + 0.010*\"caption\" + 0.010*\"copyright\" + 0.008*\"bbc\" + 0.006*\"news\" + 0.006*\"use\" + 0.006*\"peopl\" + 0.005*\"’\" + 0.005*\"said\" + 0.005*\"app\" + 0.005*\"u\" + 0.004*\"getti\" + 0.004*\"call\" + 0.004*\"may\" + 0.004*\"make\" + 0.004*\"child\" + 0.004*\"light\" + 0.004*\"n't\" + 0.004*\"medium\" + 0.004*\"work\"\n",
      "\n",
      "11: 0.012*\"news\" + 0.007*\"bbc\" + 0.006*\"happen\" + 0.006*\"wave\" + 0.005*\"break\" + 0.005*\"keep\" + 0.004*\"first\" + 0.004*\"alert\" + 0.004*\"said\" + 0.004*\"’\" + 0.004*\"woman\" + 0.004*\"peopl\" + 0.004*\"analysi\" + 0.004*\"write\" + 0.004*\"growth\" + 0.003*\"girl\" + 0.003*\"world\" + 0.003*\"need\" + 0.003*\"beetl\" + 0.003*\"http\"\n",
      "\n",
      "12: 0.016*\"’\" + 0.010*\"imag\" + 0.007*\"said\" + 0.006*\"use\" + 0.006*\"“\" + 0.005*\"”\" + 0.005*\"caption\" + 0.004*\"show\" + 0.004*\"work\" + 0.004*\"time\" + 0.004*\"peopl\" + 0.004*\"copyright\" + 0.004*\"–\" + 0.004*\"stori\" + 0.004*\"local\" + 0.004*\"u\" + 0.004*\"world\" + 0.003*\"n't\" + 0.003*\"back\" + 0.003*\"speci\"\n",
      "\n",
      "13: 0.035*\"imag\" + 0.016*\"caption\" + 0.009*\"copyright\" + 0.006*\"getti\" + 0.006*\"peopl\" + 0.005*\"said\" + 0.005*\"govern\" + 0.004*\"air\" + 0.004*\"n't\" + 0.004*\"show\" + 0.004*\"u\" + 0.003*\"bbc\" + 0.003*\"time\" + 0.003*\"princ\" + 0.003*\"duchess\" + 0.003*\"think\" + 0.003*\"child\" + 0.003*\"work\" + 0.003*\"perform\" + 0.003*\"question\"\n",
      "\n",
      "14: 0.009*\"’\" + 0.006*\"u\" + 0.006*\"bbc\" + 0.006*\"imag\" + 0.005*\"use\" + 0.005*\"medium\" + 0.005*\"call\" + 0.005*\"peopl\" + 0.005*\"make\" + 0.004*\"social\" + 0.004*\"find\" + 0.004*\"said\" + 0.004*\"work\" + 0.004*\"data\" + 0.004*\"onli\" + 0.004*\"time\" + 0.004*\"stori\" + 0.004*\"back\" + 0.003*\"n't\" + 0.003*\"need\"\n",
      "\n",
      "15: 0.012*\"said\" + 0.010*\"imag\" + 0.006*\"u\" + 0.005*\"copyright\" + 0.005*\"eu\" + 0.005*\"n't\" + 0.005*\"caption\" + 0.005*\"brexit\" + 0.004*\"’\" + 0.004*\"may\" + 0.004*\"use\" + 0.004*\"list\" + 0.004*\"light\" + 0.004*\"test\" + 0.003*\"work\" + 0.003*\"time\" + 0.003*\"anim\" + 0.003*\"child\" + 0.003*\"world\" + 0.003*\"call\"\n",
      "\n",
      "16: 0.016*\"said\" + 0.012*\"imag\" + 0.008*\"u\" + 0.007*\"’\" + 0.006*\"use\" + 0.005*\"peopl\" + 0.005*\"caption\" + 0.005*\"copyright\" + 0.005*\"time\" + 0.004*\"work\" + 0.004*\"”\" + 0.004*\"n't\" + 0.004*\"told\" + 0.004*\"call\" + 0.004*\"report\" + 0.004*\"“\" + 0.004*\"make\" + 0.003*\"take\" + 0.003*\"bbc\" + 0.003*\"stori\"\n",
      "\n",
      "17: 0.009*\"imag\" + 0.008*\"’\" + 0.007*\"said\" + 0.006*\"live\" + 0.005*\"copyright\" + 0.005*\"use\" + 0.004*\"peopl\" + 0.004*\"bbc\" + 0.004*\"caption\" + 0.004*\"govern\" + 0.004*\"u\" + 0.003*\"news\" + 0.003*\"“\" + 0.003*\"need\" + 0.003*\"work\" + 0.003*\"n't\" + 0.003*\"thing\" + 0.003*\"call\" + 0.003*\"make\" + 0.003*\"first\"\n",
      "\n",
      "18: 0.013*\"imag\" + 0.013*\"said\" + 0.006*\"’\" + 0.006*\"caption\" + 0.006*\"peopl\" + 0.005*\"u\" + 0.005*\"told\" + 0.005*\"n't\" + 0.004*\"show\" + 0.004*\"“\" + 0.004*\"copyright\" + 0.004*\"first\" + 0.004*\"”\" + 0.004*\"may\" + 0.003*\"call\" + 0.003*\"go\" + 0.003*\"world\" + 0.003*\"even\" + 0.003*\"time\" + 0.003*\"presid\"\n",
      "\n",
      "19: 0.009*\"imag\" + 0.007*\"said\" + 0.007*\"use\" + 0.007*\"u\" + 0.006*\"beetl\" + 0.005*\"“\" + 0.005*\"’\" + 0.005*\"time\" + 0.005*\"copyright\" + 0.005*\"make\" + 0.004*\"”\" + 0.004*\"bbc\" + 0.004*\"world\" + 0.004*\"peopl\" + 0.004*\"back\" + 0.003*\"caption\" + 0.003*\"light\" + 0.003*\"n't\" + 0.003*\"first\" + 0.003*\"work\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We might be able to see a pattern with some of these topics. We'd likely need to go through the corpus one more time and more move punctuation and stop words. Indeed, the first time you run an LDA, you'll need to likely re-run the LDA and move terms that happened to slip through that you need to remove. \n",
    "\n",
    "Recall our \"bag of words\" assumption. Let's again look at the first article in the corpus and see what topics make up this article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image copyright PA/EPA Image caption Oligarch Roman Abramovich (l) and PM Dmitry Medvedev are on the list\r\n",
      "\r\n",
      "Russian President Vladimir Putin says a list of officials and businessmen close to the Kremlin published by the US has in effect targeted all Russian people.\r\n",
      "\r\n",
      "The list names 210 top Russians as part of a sanctions law aimed at punishing Moscow for meddling in the US election.\r\n",
      "\r\n",
      "However, the US stressed those named were not subject to new sanctions.\r\n",
      "\r\n",
      "Mr Putin said the list was an unfriendly act that complicated US-Russia ties but he said he did not want to escalate the situation.\r\n",
      "\r\n",
      "Mr Putin said Russia should instead be thinking about \"ourselves and the economy\".\r\n",
      "\r\n",
      "The list was also derided by a number of senior Russian officials who said it bore a strong resemblance to the Forbes magazine ranking of Russian billionaires. A US Treasury Department later told Buzzfeed that an unclassified annex of the report had been derived from the magazine.\r\n",
      "\r\n",
      "Why did the US publish the list?\r\n",
      "\r\n",
      "The government was required to draw up the list after Congress passed the Countering America's Adversaries Through Sanctions Act (Caatsa) in August.\r\n",
      "\r\n",
      "The law aimed to punish Russia for its alleged meddling in the 2016 US presidential election and its actions in Ukraine.\r\n",
      "\r\n",
      "Congress wanted the list to name and shame those who had benefited from close association with President Putin and put them on notice that they could be targeted for sanctions, or more sanctions, in the future.\r\n",
      "\r\n",
      "President Donald Trump did not support Caatsa, even though he signed it into law, saying it was \"unconstitutional\".\r\n",
      "\r\n",
      "Under the law, the list had to be delivered by Monday. The fact it was released about 10 minutes before midnight may reflect Mr Trump's coolness towards it, and his opposition to punishing more Russians with sanctions.\r\n",
      "\r\n",
      "The top Democrat on the House Foreign Affairs Committee, Eliot Engel, accused the Trump administration of letting \"Russia off the hook again\" by not taking substantial action.\r\n",
      "\r\n",
      "Who has been named?\r\n",
      "\r\n",
      "Informally known as the \"Putin list\", the unclassified section has 210 names, 114 of them in the government or linked to it, or key businessmen. The other 96 are oligarchs apparently determined more by the fact they are worth more than $1bn (£710m) than their close ties to the Kremlin.\r\n",
      "\r\n",
      "Image copyright Reuters Image caption Congress passed the law in August, although President Donald Trump had opposed it\r\n",
      "\r\n",
      "Most of Mr Putin's longstanding allies are named, many of them siloviki (security guys). They include the spy chiefs Alexander Bortnikov of the Federal Security Service (FSB) - which Mr Putin used to run - and Sergei Naryshkin of the Foreign Intelligence Service (SVR).\r\n",
      "\r\n",
      "The men who control Russia's energy resources are listed: Gazprom chief Alexei Miller, Rosneft chief Igor Sechin and other oil and gas executives, along with top bankers like Bank Rossiya manager Yuri Kovalchuk.\r\n",
      "\r\n",
      "The oligarchs include Kirill Shamalov, who is reported to be Mr Putin's son-in-law, although the Kremlin has never confirmed his marriage to Katerina Tikhonova, nor even that she is the president's daughter.\r\n",
      "\r\n",
      "Internationally known oligarchs are there too, such as those with stakes in top English football clubs: Alisher Usmanov (Arsenal) and Roman Abramovich (Chelsea).\r\n",
      "\r\n",
      "Will they face new sanctions?\r\n",
      "\r\n",
      "Not at the moment. The US Treasury document itself stresses: \"It is not a sanctions list, and the inclusion of individuals or entities... does not and in no way should be interpreted to impose sanctions on those individuals or entities.\"\r\n",
      "\r\n",
      "It adds: \"Neither does inclusion on the unclassified list indicate that the US government has information about the individual's involvement in malign activities.\"\r\n",
      "\r\n",
      "However, there is a classified version said to include information detailing allegations of involvement in corrupt activities.\r\n",
      "\r\n",
      "What does it mean for Russia's elite?\r\n",
      "\r\n",
      "Analysis: Steve Rosenberg, BBC Moscow correspondent\r\n",
      "\r\n",
      "The good news for the Kremlin: this isn't a sanctions list. But the good news ends there.\r\n",
      "\r\n",
      "Those Russian officials and oligarchs named by the US Treasury will worry that their inclusion could signal sanctions in the future.\r\n",
      "\r\n",
      "Even before the list was made public, the Kremlin had claimed the US Treasury report was an attempt to meddle in Russia's presidential election.\r\n",
      "\r\n",
      "The list reads like a Who's Who of the Russian political elite and business world.\r\n",
      "\r\n",
      "Moscow won't want that to become a Who's Sanctioned.\r\n",
      "\r\n",
      "What is the Caatsa act and did the president want it?\r\n",
      "\r\n",
      "The law limited the amount of money Americans could invest in Russian energy projects and made it more difficult for US companies to do business with Russia.\r\n",
      "\r\n",
      "It also imposed sanctions on Iran and North Korea.\r\n",
      "\r\n",
      "Media playback is unsupported on your device Media caption All you need to know about the Trump-Russia investigation\r\n",
      "\r\n",
      "In signing the act, Mr Trump attached a statement calling the measure \"deeply flawed\" and said he could make \"far better deals with foreign countries than Congress\".\r\n",
      "\r\n",
      "Earlier on Monday, the US government argued the Caatsa law had already pushed governments around the world to cancel deals with Russia worth billions, suggesting that more sanctions were not required.\r\n",
      "\r\n",
      "How have the Russians reacted?\r\n",
      "\r\n",
      "Perhaps referring to the fact that all of their political representatives had been named, Mr Putin said that, in effect, \"all 146 million Russians have been put on the list\".\r\n",
      "\r\n",
      "He joked he was offended not to be named himself.\r\n",
      "\r\n",
      "Earlier, Kremlin spokesman Dmitry Peskov, who is himself on the list, accepted that it was not one of sanctions but said it could potentially damage \"the image and reputation\" of figures listed and their associated companies.\r\n",
      "\r\n",
      "He added: \"It's not the first day that we live with quite aggressive comments made towards us, so we should not give in to emotions.\"\r\n",
      "\r\n",
      "When Caatsa was passed, PM Dmitry Medvedev said it meant the US had declared a \"full-scale trade war\" on Russia.\r\n",
      "\r\n",
      "Russian opposition leader Alexei Navalny praised the publication of the names as \"a good list\".\n"
     ]
    }
   ],
   "source": [
    "print(data.articles.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this article has to do with Russia and U.S. sanctions. We can use the LDA model we just ran and see what topics make up this article. \n",
    "\n",
    "Let's use `lda_model` and pass in the text from `corpus`, namely the first document, to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 0.8298334), (16, 0.15275174), (18, 0.017057694)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model[corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This produces a list of tuples. The first value is the ID related to the topic and the second value is how much this topic dominates the document.\n",
    "\n",
    "Look at what the top terms for each topic is (as produced by the `for loop`). Does this make sense to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 5 of 7\n",
    "## Now you try!\n",
    "\n",
    "### Run the LDA model for your NY Times data. Choose some `K` number of topics. I would suggest pick a value of `K` that is less than 20 but greater than 5. (Let's keep `alpha` as 0.01.) Save your LDA model as `NYTimes_lda_model`. \n",
    "\n",
    "### Print out the top twenty words for each topic. \n",
    "\n",
    "### Which topics make up first article in your corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_lda_model = models.LdaMulticore(NYTimes_corpus,\\\n",
    "                            num_topics=num_topics,\\\n",
    "                                  id2word=NYTimes_dictionary_LDA,\\\n",
    "                                  alpha=[0.01]*num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.004*\"show\" + 0.004*\"compani\" + 0.003*\"school\" + 0.003*\"home\" + 0.003*\"citi\" + 0.003*\"child\" + 0.003*\"three\" + 0.002*\"music\" + 0.002*\"art\" + 0.002*\"live\" + 0.002*\"becaus\" + 0.002*\"danc\" + 0.002*\"street\" + 0.002*\"name\" + 0.002*\"season\" + 0.002*\"museum\" + 0.002*\"made\" + 0.002*\"perform\" + 0.002*\"might\" + 0.002*\"coach\"\n",
      "\n",
      "1: 0.013*\"compani\" + 0.005*\"busi\" + 0.004*\"percent\" + 0.003*\"citi\" + 0.003*\"googl\" + 0.003*\"servic\" + 0.003*\"market\" + 0.003*\"help\" + 0.003*\"thing\" + 0.003*\"manag\" + 0.003*\"million\" + 0.002*\"still\" + 0.002*\"becaus\" + 0.002*\"american\" + 0.002*\"billion\" + 0.002*\"play\" + 0.002*\"report\" + 0.002*\"season\" + 0.002*\"part\" + 0.002*\"execut\"\n",
      "\n",
      "2: 0.008*\"compani\" + 0.006*\"uber\" + 0.005*\"googl\" + 0.004*\"citi\" + 0.004*\"game\" + 0.004*\"insur\" + 0.004*\"servic\" + 0.003*\"busi\" + 0.003*\"boston\" + 0.003*\"world\" + 0.003*\"onli\" + 0.003*\"home\" + 0.003*\"take\" + 0.003*\"name\" + 0.003*\"olymp\" + 0.002*\"technolog\" + 0.002*\"percent\" + 0.002*\"call\" + 0.002*\"public\" + 0.002*\"show\"\n",
      "\n",
      "3: 0.007*\"compani\" + 0.004*\"servic\" + 0.003*\"name\" + 0.003*\"busi\" + 0.003*\"million\" + 0.003*\"googl\" + 0.003*\"internet\" + 0.003*\"citi\" + 0.002*\"call\" + 0.002*\"part\" + 0.002*\"show\" + 0.002*\"execut\" + 0.002*\"world\" + 0.002*\"made\" + 0.002*\"much\" + 0.002*\"home\" + 0.002*\"still\" + 0.002*\"onli\" + 0.002*\"coach\" + 0.002*\"street\"\n",
      "\n",
      "4: 0.005*\"citi\" + 0.005*\"million\" + 0.004*\"compani\" + 0.004*\"game\" + 0.003*\"team\" + 0.003*\"season\" + 0.003*\"onli\" + 0.003*\"world\" + 0.003*\"name\" + 0.003*\"coach\" + 0.003*\"play\" + 0.003*\"percent\" + 0.002*\"week\" + 0.002*\"made\" + 0.002*\"call\" + 0.002*\"home\" + 0.002*\"part\" + 0.002*\"american\" + 0.002*\"unit\" + 0.002*\"take\"\n",
      "\n",
      "5: 0.005*\"compani\" + 0.004*\"citi\" + 0.003*\"call\" + 0.003*\"home\" + 0.003*\"servic\" + 0.002*\"much\" + 0.002*\"game\" + 0.002*\"three\" + 0.002*\"percent\" + 0.002*\"unit\" + 0.002*\"week\" + 0.002*\"offic\" + 0.002*\"countri\" + 0.002*\"take\" + 0.002*\"nation\" + 0.002*\"world\" + 0.002*\"public\" + 0.002*\"right\" + 0.002*\"help\" + 0.002*\"name\"\n",
      "\n",
      "6: 0.004*\"game\" + 0.003*\"compani\" + 0.003*\"team\" + 0.003*\"percent\" + 0.003*\"onli\" + 0.003*\"boston\" + 0.003*\"busi\" + 0.003*\"world\" + 0.003*\"season\" + 0.003*\"bowl\" + 0.002*\"much\" + 0.002*\"hotel\" + 0.002*\"help\" + 0.002*\"citi\" + 0.002*\"made\" + 0.002*\"run\" + 0.002*\"american\" + 0.002*\"becaus\" + 0.002*\"take\" + 0.002*\"manag\"\n",
      "\n",
      "7: 0.005*\"compani\" + 0.005*\"citi\" + 0.003*\"percent\" + 0.003*\"game\" + 0.003*\"million\" + 0.003*\"american\" + 0.002*\"still\" + 0.002*\"street\" + 0.002*\"take\" + 0.002*\"onli\" + 0.002*\"art\" + 0.002*\"call\" + 0.002*\"season\" + 0.002*\"univers\" + 0.002*\"becaus\" + 0.002*\"may\" + 0.002*\"show\" + 0.002*\"well\" + 0.002*\"school\" + 0.002*\"public\"\n",
      "\n",
      "8: 0.003*\"show\" + 0.003*\"made\" + 0.003*\"art\" + 0.003*\"compani\" + 0.003*\"season\" + 0.003*\"onli\" + 0.002*\"play\" + 0.002*\"help\" + 0.002*\"becaus\" + 0.002*\"republican\" + 0.002*\"three\" + 0.002*\"back\" + 0.002*\"still\" + 0.002*\"citi\" + 0.002*\"museum\" + 0.002*\"call\" + 0.002*\"american\" + 0.002*\"well\" + 0.002*\"project\" + 0.002*\"public\"\n",
      "\n",
      "9: 0.005*\"compani\" + 0.003*\"citi\" + 0.003*\"game\" + 0.003*\"public\" + 0.003*\"week\" + 0.003*\"made\" + 0.003*\"unit\" + 0.002*\"call\" + 0.002*\"season\" + 0.002*\"onli\" + 0.002*\"school\" + 0.002*\"live\" + 0.002*\"percent\" + 0.002*\"nation\" + 0.002*\"industri\" + 0.002*\"busi\" + 0.002*\"million\" + 0.002*\"plan\" + 0.002*\"boston\" + 0.002*\"three\"\n",
      "\n",
      "10: 0.006*\"compani\" + 0.004*\"republican\" + 0.004*\"play\" + 0.004*\"game\" + 0.003*\"team\" + 0.003*\"million\" + 0.003*\"citi\" + 0.003*\"three\" + 0.003*\"run\" + 0.003*\"season\" + 0.003*\"bowl\" + 0.003*\"becaus\" + 0.003*\"senat\" + 0.002*\"super\" + 0.002*\"back\" + 0.002*\"call\" + 0.002*\"may\" + 0.002*\"presid\" + 0.002*\"onli\" + 0.002*\"win\"\n",
      "\n",
      "11: 0.004*\"name\" + 0.004*\"compani\" + 0.004*\"call\" + 0.003*\"citi\" + 0.003*\"game\" + 0.003*\"team\" + 0.003*\"play\" + 0.003*\"onli\" + 0.002*\"accord\" + 0.002*\"show\" + 0.002*\"busi\" + 0.002*\"bowl\" + 0.002*\"made\" + 0.002*\"never\" + 0.002*\"art\" + 0.002*\"becaus\" + 0.002*\"take\" + 0.002*\"wine\" + 0.002*\"season\" + 0.002*\"drug\"\n",
      "\n",
      "12: 0.005*\"art\" + 0.003*\"made\" + 0.003*\"museum\" + 0.003*\"show\" + 0.003*\"american\" + 0.003*\"back\" + 0.002*\"compani\" + 0.002*\"live\" + 0.002*\"school\" + 0.002*\"citi\" + 0.002*\"home\" + 0.002*\"call\" + 0.002*\"think\" + 0.002*\"week\" + 0.002*\"presid\" + 0.002*\"street\" + 0.002*\"come\" + 0.002*\"place\" + 0.002*\"never\" + 0.002*\"becaus\"\n",
      "\n",
      "13: 0.004*\"wine\" + 0.003*\"show\" + 0.003*\"compani\" + 0.003*\"name\" + 0.003*\"onli\" + 0.003*\"american\" + 0.003*\"live\" + 0.003*\"produc\" + 0.003*\"life\" + 0.002*\"becaus\" + 0.002*\"report\" + 0.002*\"well\" + 0.002*\"call\" + 0.002*\"california\" + 0.002*\"broadway\" + 0.002*\"citi\" + 0.002*\"around\" + 0.002*\"much\" + 0.002*\"music\" + 0.002*\"may\"\n",
      "\n",
      "14: 0.005*\"colleg\" + 0.004*\"coach\" + 0.004*\"game\" + 0.004*\"season\" + 0.004*\"school\" + 0.003*\"team\" + 0.003*\"take\" + 0.002*\"back\" + 0.002*\"start\" + 0.002*\"think\" + 0.002*\"becaus\" + 0.002*\"ask\" + 0.002*\"job\" + 0.002*\"citi\" + 0.002*\"made\" + 0.002*\"thing\" + 0.002*\"univers\" + 0.002*\"end\" + 0.002*\"much\" + 0.002*\"play\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in NYTimes_lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jessica Mah was 20 when she helped found InDinero in 2009. Back then she believed she could help small-business owners track their finances with her start-ups software. But as it turned out, Ms. Mah could barely keep InDinero afloat, let alone help others run their businesses. In its first version, InDinero, in San Francisco, charged its few customers $20 a month for its software. Competition, which included companies like QuickBooks, was fierce, and a series of bad hires added to InDineros woes. The company was going nowhere, Ms. Mah said. But by the time she realized this, in 2012, InDinero had spent nearly all of the $1.2 million it had raised from investors. A few years ago, I really didnt know that much, said Ms. Mah, the chief executive. In fact, she said, I almost crashed the company. And yet, today, InDinero, which provides accounting software and services to small businesses, has 75 employees and just under $3 million in annual sales. It expects to double that in 2015, and has raised a total of $8 million from investors. In a recent conversation, which has been edited and condensed, Ms. Mah talked about how she turned things around. Q. What were the first signs of a problem? A. I tracked the metrics  the number of sign-ups, upgrades and cancellations. The numbers that should have been going up werent. Q. How close did you come to running out of money? A. We were down to the last $150,000 of the $1.2 million we had raised, and were burning $80,000 to $100,000 a month. Once we got down to about $250,000 we knew we had to dial back. Q. What steps did you take once you realized InDinero was in serious trouble? A. We got rid of our office and employees so we had no expenses. My co-founder, Andy Su, and I moved into an apartment together and got our parents to cover our rent and food. I would have moved home, but my family is in New York, and the company is in California. Then, for most of 2012, we tried to figure out what magical feature we could build to fix InDinero. Q. Did you figure it out? A. I started asking, What would InDinero look like if it were a $1 billion-plus company? Looking at companies like Salesforce and NetSuite, I learned a few things. They all solve a big business problem that justifies a price point higher than $20 a month. They have seasoned executives and well-trained sales teams. Theyre nearly 100 percent premium, with very limited free offerings. Q. How did this help you change your product? A. We knew we needed a product that we could charge a few hundred dollars a month for. I spent countless hours interviewing entrepreneurs of all different shapes and sizes to figure out their accounting and tax needs. We knew they wanted a one-stop accounting solution. It was a big problem for them that we didnt file their taxes. We had to go from offering a cheap software solution that didnt actually solve any problems to being an all-in-one accounting back office with accountants on staff. We had to do it all, including taxes and payroll. Q. How big a business can your software handle  especially when it comes to filing taxes? A. Our 500 customers range from a two-employee start-up with no sales to a 100-employee company with eight-figure sales. Q. What do you charge? A. Businesses pay between $400 and $5,000 per month, depending on how complicated their accounting is. Q. How are you marketing your service? A. Weve found the best way to get new business is to incentivize our current customers. For every referral a customer gives us, we give them a free month. Weve found that, out of five customers, four wont send any referrals, but one will send 50. Q. Once you had the right product, how did you go about hiring the right people? A. I thought about the mistakes I made the first time. I realized I had hired too many of my friends. I should have spent more time evaluating candidates outside my network  expanding the candidate pool through external recruiters, LinkedIn, specialty job boards and other methods. My original interview process wasnt thorough. I had no defined criteria to evaluate candidates against. Q. Do you have any favorite interview questions? A. Yes, I ask the same ones to every single candidate. Some are, Whats the hardest youve ever worked in your life, whats your most lofty ambition and what are you doing for self-development? I want to know what books and blogs they read, what conferences they go to, whether theyre working on side projects in their fields and whether theyve ever run a company. I want a company full of mini-C.E.O.s. I also ask about their relationship with their last boss. If they talk about how horrible their last boss was, Im done. Q. Has your management style changed since InDineros early days? A. Yes, Im focused on going from being a C.E.O. who did the work herself to an effective leader. Im taking the personal growth thing very seriously. Ive hired executive coaches and joined Y.P.O. [Young Presidents Organization]. Last year, I read more than 100 leadership books. Q. Is it hard to let go of the day-to-day? A. In the past, I didnt spend enough time recruiting for senior leadership. I tried to manage 20 people all by myself with no strong managers to grow the company. Now, I dont spend any time having individual contributors report to me. I go straight to finding a strong V.P./director-level person who can build the team out for me. Q. Did InDineros early problems test your relationship with your co-founder? A. Andy and I have always had a strong relationship. Were good friends  and still roommates. Living together is key because we can work on business challenges at all hours of the day. I dont see this changing anytime soon. Q. Whats the division of labor between you? A. Im responsible for customer happiness and customer acquisition. Andy, the chief technology officer, is responsible for product and engineering. We split all the other business functions  like legal, finance and operations  50-50. Q. How do you resolve disputes? A. If we disagree on how to deal with something, we ask the other person how strongly they feel on a 1-to-10 scale about that particular issue. The person who cares more will make the decision. We also see an executive coach to help mediate our disagreements. On top of that, we go to co-founder marriage counseling! [The co-founders are not romantically involved.] One of our core company values is rethink the obvious, so I got the idea to reach out to marriage counselors listed on Yelp to see if any would work with us. Even though marriage counseling is usually for dysfunctional couples, co-founders often have the same petty debates. We wanted to be proactive. Q. What have you learned about early success  and failure? A. In the beginning, because of my age, I got a lot of wunderkind attention. Its important to stay humble and not get carried away by early success  but the same is true of failure. Neither necessarily lasts. So, when InDinero was in trouble, I just talked to my parents and friends, lay low and focused on results.\n"
     ]
    }
   ],
   "source": [
    "print(NYTimes_df.article.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9997056)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYTimes_lda_model[corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Predicting topics on unseen documents\n",
    "\n",
    "Let's try and predict the topics on a document unseen by the LDA (i.e., not trained by the LDA). Here is a sample document about Twitter. Let's see what topics are found in this unseen article by the LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document = '''Eric Tucker, a 35-year-old co-founder of a marketing company in Austin, Tex., had just about 40 Twitter followers. But his recent tweet about paid protesters being bused to demonstrations against President-elect Donald J. Trump fueled a nationwide conspiracy theory — one that Mr. Trump joined in promoting. \n",
    "\n",
    "Mr. Tucker's post was shared at least 16,000 times on Twitter and more than 350,000 times on Facebook. The problem is that Mr. Tucker got it wrong. There were no such buses packed with paid protesters.\n",
    "\n",
    "But that didn't matter.\n",
    "\n",
    "While some fake news is produced purposefully by teenagers in the Balkans or entrepreneurs in the United States seeking to make money from advertising, false information can also arise from misinformed social media posts by regular people that are seized on and spread through a hyperpartisan blogosphere.\n",
    "\n",
    "Here, The New York Times deconstructs how Mr. Tucker’s now-deleted declaration on Twitter the night after the election turned into a fake-news phenomenon. It is an example of how, in an ever-connected world where speed often takes precedence over truth, an observation by a private citizen can quickly become a talking point, even as it is being proved false.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(document) #Turn the document into a list of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eric',\n",
       " 'Tucker',\n",
       " ',',\n",
       " 'a',\n",
       " '35-year-old',\n",
       " 'co-founder',\n",
       " 'of',\n",
       " 'a',\n",
       " 'marketing',\n",
       " 'company',\n",
       " 'in',\n",
       " 'Austin',\n",
       " ',',\n",
       " 'Tex.',\n",
       " ',',\n",
       " 'had',\n",
       " 'just',\n",
       " 'about',\n",
       " '40',\n",
       " 'Twitter',\n",
       " 'followers',\n",
       " '.',\n",
       " 'But',\n",
       " 'his',\n",
       " 'recent',\n",
       " 'tweet',\n",
       " 'about',\n",
       " 'paid',\n",
       " 'protesters',\n",
       " 'being',\n",
       " 'bused',\n",
       " 'to',\n",
       " 'demonstrations',\n",
       " 'against',\n",
       " 'President-elect',\n",
       " 'Donald',\n",
       " 'J.',\n",
       " 'Trump',\n",
       " 'fueled',\n",
       " 'a',\n",
       " 'nationwide',\n",
       " 'conspiracy',\n",
       " 'theory',\n",
       " '—',\n",
       " 'one',\n",
       " 'that',\n",
       " 'Mr.',\n",
       " 'Trump',\n",
       " 'joined',\n",
       " 'in',\n",
       " 'promoting',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Tucker',\n",
       " \"'s\",\n",
       " 'post',\n",
       " 'was',\n",
       " 'shared',\n",
       " 'at',\n",
       " 'least',\n",
       " '16,000',\n",
       " 'times',\n",
       " 'on',\n",
       " 'Twitter',\n",
       " 'and',\n",
       " 'more',\n",
       " 'than',\n",
       " '350,000',\n",
       " 'times',\n",
       " 'on',\n",
       " 'Facebook',\n",
       " '.',\n",
       " 'The',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'Mr.',\n",
       " 'Tucker',\n",
       " 'got',\n",
       " 'it',\n",
       " 'wrong',\n",
       " '.',\n",
       " 'There',\n",
       " 'were',\n",
       " 'no',\n",
       " 'such',\n",
       " 'buses',\n",
       " 'packed',\n",
       " 'with',\n",
       " 'paid',\n",
       " 'protesters',\n",
       " '.',\n",
       " 'But',\n",
       " 'that',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'matter',\n",
       " '.',\n",
       " 'While',\n",
       " 'some',\n",
       " 'fake',\n",
       " 'news',\n",
       " 'is',\n",
       " 'produced',\n",
       " 'purposefully',\n",
       " 'by',\n",
       " 'teenagers',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Balkans',\n",
       " 'or',\n",
       " 'entrepreneurs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'seeking',\n",
       " 'to',\n",
       " 'make',\n",
       " 'money',\n",
       " 'from',\n",
       " 'advertising',\n",
       " ',',\n",
       " 'false',\n",
       " 'information',\n",
       " 'can',\n",
       " 'also',\n",
       " 'arise',\n",
       " 'from',\n",
       " 'misinformed',\n",
       " 'social',\n",
       " 'media',\n",
       " 'posts',\n",
       " 'by',\n",
       " 'regular',\n",
       " 'people',\n",
       " 'that',\n",
       " 'are',\n",
       " 'seized',\n",
       " 'on',\n",
       " 'and',\n",
       " 'spread',\n",
       " 'through',\n",
       " 'a',\n",
       " 'hyperpartisan',\n",
       " 'blogosphere',\n",
       " '.',\n",
       " 'Here',\n",
       " ',',\n",
       " 'The',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Times',\n",
       " 'deconstructs',\n",
       " 'how',\n",
       " 'Mr.',\n",
       " 'Tucker',\n",
       " '’',\n",
       " 's',\n",
       " 'now-deleted',\n",
       " 'declaration',\n",
       " 'on',\n",
       " 'Twitter',\n",
       " 'the',\n",
       " 'night',\n",
       " 'after',\n",
       " 'the',\n",
       " 'election',\n",
       " 'turned',\n",
       " 'into',\n",
       " 'a',\n",
       " 'fake-news',\n",
       " 'phenomenon',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'how',\n",
       " ',',\n",
       " 'in',\n",
       " 'an',\n",
       " 'ever-connected',\n",
       " 'world',\n",
       " 'where',\n",
       " 'speed',\n",
       " 'often',\n",
       " 'takes',\n",
       " 'precedence',\n",
       " 'over',\n",
       " 'truth',\n",
       " ',',\n",
       " 'an',\n",
       " 'observation',\n",
       " 'by',\n",
       " 'a',\n",
       " 'private',\n",
       " 'citizen',\n",
       " 'can',\n",
       " 'quickly',\n",
       " 'become',\n",
       " 'a',\n",
       " 'talking',\n",
       " 'point',\n",
       " ',',\n",
       " 'even',\n",
       " 'as',\n",
       " 'it',\n",
       " 'is',\n",
       " 'being',\n",
       " 'proved',\n",
       " 'false',\n",
       " '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass in this new article and see what topics comprise it according to the LDA. \n",
    "\n",
    "We can use the `lda_model` and `dictionary_LDA` object and its `.doc2bow()` method, to take each token from `document` to calculate the topic distribution. \n",
    "\n",
    "Here, I've saved the output as a `DataFrame` to make it easier to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic #</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic #  weight\n",
       "0       10    0.17\n",
       "1       16    0.82"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([(el[0], round(el[1],2)) for el in lda_model[dictionary_LDA.doc2bow(tokens)]], columns=['topic #', 'weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here the distribution of topics across the documents. Note, that the weight sums to 100%\n",
    "\n",
    "Given the article, look back at the top terms for each of these topics and see if you agree with this distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Visualizing Topics\n",
    "\n",
    "`pyLDAvis` is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.\n",
    "\n",
    "The visualization is intended to be used within a Jupyter notebook but can also be saved to a stand-alone HTML file for easy sharing.\n",
    "\n",
    "First, we need to install `pyLDAvis`. We'll also need to install `joblib` to help us run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: pyLDAvis in /home/ahegu/.local/lib/python3.6/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /usr/lib/python3.6/site-packages (from pyLDAvis) (1.17.3)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /usr/lib/python3.6/site-packages (from pyLDAvis) (0.33.6)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /usr/lib/python3.6/site-packages (from pyLDAvis) (2.10.3)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /usr/lib/python3.6/site-packages (from pyLDAvis) (1.3.1)\n",
      "Requirement already satisfied: funcy in /home/ahegu/.local/lib/python3.6/site-packages (from pyLDAvis) (1.14)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /usr/lib/python3.6/site-packages (from pyLDAvis) (0.25.2)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /usr/lib/python3.6/site-packages (from pyLDAvis) (0.14.0)\n",
      "Requirement already satisfied: pytest in /usr/lib/python3.6/site-packages (from pyLDAvis) (5.2.2)\n",
      "Requirement already satisfied: numexpr in /usr/lib/python3.6/site-packages (from pyLDAvis) (2.7.0)\n",
      "Requirement already satisfied: future in /usr/lib/python3.6/site-packages (from pyLDAvis) (0.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/lib/python3.6/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/lib/python3.6/site-packages (from pytest->pyLDAvis) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/lib/python3.6/site-packages (from pytest->pyLDAvis) (0.23)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.6/site-packages (from pytest->pyLDAvis) (19.2)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.8.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/lib/python3.6/site-packages (from pytest->pyLDAvis) (4.3.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/lib/python3.6/site-packages (from pytest->pyLDAvis) (0.13.0)\n",
      "Requirement already satisfied: wcwidth in /usr/lib/python3.6/site-packages (from pytest->pyLDAvis) (0.1.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.17.0->pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3.6/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->pyLDAvis) (0.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3.6/site-packages (from packaging->pytest->pyLDAvis) (2.4.2)\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: joblib in /usr/lib/python3.6/site-packages (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3.6 install --user pyLDAvis\n",
    "!pip3.6 install --user joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import these modules, and set `%matplotlib in line` to visualize the LDA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the pyLDAvis! Thankfully, we already have all the inputs that we need from our previous work. \n",
    "\n",
    "One somewhat confusing thing is that we need add in an extra line of code `with parallel_backend('threading'):` in order for it to work. This is because `pyLDAvis` is very CPU and memory intensive, and parallelizing it in the backend will ensure the visualization works and doesn't crash. \n",
    "\n",
    "**NOTE:** This may take a few minutes to run! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-85f4954d558a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'threading'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary_LDA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    396\u001b[0m    \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m    \u001b[0mtopic_info\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m    \u001b[0mtoken_table\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m    \u001b[0mtopic_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m    top_terms = pd.concat(Parallel(n_jobs=n_jobs)(delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls) \\\n\u001b[0;32m--> 255\u001b[0;31m                                                  for ls in _job_chunks(lambda_seq, n_jobs)))\n\u001b[0m\u001b[1;32m    256\u001b[0m    \u001b[0mtopic_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_top_term_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_term_info\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary_LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a short vignette that explains more specifically what's happening: \n",
    "\n",
    "https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf\n",
    "\n",
    "In broad strokes, it visualizes the topics that were found in the LDA model. \n",
    "\n",
    "First in the box next to \"Selectec Topic\" put in a topic number (anywhere from 0 to 19, inclusive) to visualize the topic. \n",
    "\n",
    "For each topic, the visualization produces two plots. The plot to the left is the \"intertopic distance\" map, or how dissimilar or similar topics are to one another. (This procedure actually uses a PCA!) The size of each topic's \"bubble\" is proportional to the proportions of the topics across all of the tokens in the corpus. \n",
    "\n",
    "The bar chart on th right is the most relevant topics to each topic. The red bars estimate number of times a given term was generated by a given topic. The blue bars capture the overall frequency of each term in the corpus. Finally, the relevance of words is computed with a parameter lambda, where the optimal Lambda value is taken at around ~0.6 (https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 6 of 7\n",
    "## Now you try!\n",
    "\n",
    "### Use `pyLDAvis` for the LDA model you ran on the New York Times data. Explore a few topics. Remember, everyone will pick a different value of `K`, so the results will vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend('threading'):\n",
    "    vis = pyLDAvis.gensim.prepare(topic_model=NYTimes_lda_model, corpus=NYTimes_corpus, dictionary=NYTimes_dictionary_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Validating our LDA Model | What Value of `K` is Best?\n",
    "\n",
    "There are many other approaches to evaluate topci models, but most approaches are rather poor. One way is just human inspection, to see if these topics make sense. Hence, topic visualization is a good way to assess topic models. \n",
    "\n",
    "However, can we find some sort of quantifiable metric? One measure that is commonly used is perplexity, a measure of entropy.\n",
    "\n",
    "Perplexity as well is one of the intrinsic evaluation metric, and is widely used for language model evaluation. It captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set. \n",
    "\n",
    "Focussing on the log-likelihood part, you can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data? However, recent studies have shown that predictive likelihood (or equivalently, perplexity) and human judgment are often not correlated, and even sometimes slightly anti-correlated. In other words, optimizing for perplexity may not yield human interpretable topics. \n",
    "\n",
    "#### Topic Coherence\n",
    "The concept of topic coherence combines a number of measures into a framework to evaluate the coherence between topics inferred by a model. \n",
    "\n",
    "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference.\n",
    "\n",
    "Coherence is a set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is “the game is a team sport”, “the game is played with a ball”, “the game demands great physical efforts”\n",
    "\n",
    "There are several ways to measure coherence. We'll use a method called `c_v`. It's a measure based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity. \n",
    "\n",
    "Two other coherence measures are:\n",
    "        1. `C_uci` measure is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words\n",
    "        2. `C_umass` is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure\n",
    "\n",
    "First, let's import `CoherenceModel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the coherence score. We've run everything that we need already, so it's a matter of putting it into our function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model,\\\n",
    "                                     texts=initial_corpus,\\\n",
    "                                     dictionary=dictionary_LDA,\\\n",
    "                                     coherence='c_v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the `.get_coherence()` method from our coherence object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.261952131824161"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a rather high coherence value! That's great. However, is it optimal? We want the lowest number of `K` topics while also yielding the highest coherence values. \n",
    "\n",
    "If you recall our `GridSearch` approach, we can perform a similar task here. We can actually test this out here. ** However,** it'll require a lot of processing power! \n",
    "\n",
    "So, for our purposes here, we won't try it. That said, I provide code below where you can pass in the various parameters of the LDA model and it will calculate coherence scores. \n",
    "\n",
    "As with most machine learning models, you want to minimize `K` while yielding the highest value for coherence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=5):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics_K in range(start, limit, step):\n",
    "        lda_model_K = models.LdaMulticore(corpus,\\\n",
    "                            num_topics=num_topics_K,\\\n",
    "                                  id2word=dictionary,\\\n",
    "                                  alpha=[0.01]*num_topics_K)\n",
    "        \n",
    "        model_list.append(lda_model_K)\n",
    "        coherencemodel = CoherenceModel(model=lda_model_K, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list, coherence_values = compute_coherence_values(dictionary=dictionary_LDA,\\\n",
    "#                                                        corpus=corpus,\\\n",
    "#                                                        texts=initial_corpus,\\\n",
    "#                                                        start=5,\\\n",
    "#                                                        limit=30,\\\n",
    "#                                                        step=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then map out these values and plot them to find what the best coherence score might be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#limit=30; start=5; step=5;\n",
    "#x = range(start, limit, step)\n",
    "#plt.plot(x, coherence_values)\n",
    "#plt.xlabel(\"Num Topics\")\n",
    "#plt.ylabel(\"Coherence score\")\n",
    "#plt.legend((\"coherence_values\"), loc='best')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 7 of 7\n",
    "## Now you try!\n",
    "\n",
    "### Re-run the LDA model with the New York Times data with two new different values of `K` (one larger than the decision you picked before and one smaller) and calculate the coherence scores for the two new values of `K`. (DO NOT use the function above, unless you have lots of time!) \n",
    "\n",
    "### Based on the coherence scores, which of the three values of `K` is optimal?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "NYTimes_coherence_model_lda = CoherenceModel(model=NYTimes_lda_model,\\\n",
    "                                     texts=NYTimes_initial_corpus,\\\n",
    "                                     dictionary=NYTimes_dictionary_LDA,\\\n",
    "                                     coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#k=10\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_lda_model_k10 = models.LdaMulticore(NYTimes_corpus,\\\n",
    "                            num_topics=num_topics,\\\n",
    "                                  id2word=NYTimes_dictionary_LDA,\\\n",
    "                                  alpha=[0.01]*num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "NYTimes_coherence_model_lda_k10 = CoherenceModel(model=NYTimes_lda_model_k10,\\\n",
    "                                     texts=NYTimes_initial_corpus,\\\n",
    "                                     dictionary=NYTimes_dictionary_LDA,\\\n",
    "                                     coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_coherence_lda_k10 = NYTimes_coherence_model_lda_k10.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#k=20\n",
    "num_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_lda_model_k20 = models.LdaMulticore(NYTimes_corpus,\\\n",
    "                            num_topics=num_topics,\\\n",
    "                                  id2word=NYTimes_dictionary_LDA,\\\n",
    "                                  alpha=[0.01]*num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "NYTimes_coherence_model_lda_k20 = CoherenceModel(model=NYTimes_lda_model_k20,\\\n",
    "                                     texts=NYTimes_initial_corpus,\\\n",
    "                                     dictionary=NYTimes_dictionary_LDA,\\\n",
    "                                     coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NYTimes_coherence_lda_k20 = NYTimes_coherence_model_lda_k20.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=15\n",
    "NYTimes_coherence_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYTimes_coherence_lda_k10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYTimes_coherence_lda_k20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k=15 yields to the highest coherence value among the three different values of k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
